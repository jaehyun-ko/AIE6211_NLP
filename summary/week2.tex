\documentclass[
	11pt,
	a4paper,
	% kosection,
	% footnote,
	% nobookmarks,
	% microtype,
	figtabcapt,
%	lwarp
]{oblivoir}

%%%%%%%%%%%%%%%%
% Front Matter %
%%%%%%%%%%%%%%%%

\input{setup/packages}							% Declare packages

%%%%%%%%%%
% Title, Authors, Date %
%%%%%%%%%%

\title{\vspace{-4cm}Week 2 : Seq2Seq Attention for Neural Machine Translation}
\author{120220210 고재현}
\date{\today}

\begin{document}

%%%%%%%%%%
% covers %
%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%
% Table of Contents %
%%%%%%%%%%%%%%%%%%%%%
\maketitle

\pagenumbering{roman}                           % Start page numbering in Roman numerals
% \tableofcontents*        						% Add table of contents
% \clearpage

%%%%%%%%%%%%%%%%%
% preliminaries %
%%%%%%%%%%%%%%%%%

\setcounter{table}{0}		                    % Reset table counter
\setcounter{figure}{0}		                    % Reset figure counter

%%%%%%%%%%%%%
% Main Text %
%%%%%%%%%%%%%

\pagenumbering{arabic}							% Start page numbering in Arabic numerals

\section{Background: Neural Machine Translation}
확률론적 관점에서,
기계번역이란 주어진 문장을 조건으로 하는 생성될 문장의 확률을 최대화하는 문장의 값을 구하는 문제이다.
우리는 문장을 토큰화함으로써 입력 단어열을 연속확률공간속의 입력요소로 볼 수 있고,
입력과 출력 사이의 조건부확률을 RNN을 이용하여 모델링함으로써 문장을 생성할 수 있다.

RNN만을 사용하면 이전의 입력에 대한 정보를 잘 반영하지 못한다는 단점이 있다.
이를 해결하기 위해서 Attention이 추가된 Seq2Seq 모델을 사용한다.

%%%%%%%%%%%%%%%%%%%%%%
% References Section %
%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{ieeetr}						% Declare bibliography style
\bibliography{week2.bib}							% Declare bibliography file

\clearpage

\end{document}
